{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input & Output State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class ChatMessages(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    llm_calls: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: ChatMessages):\n",
    "    question = state[\"question\"]\n",
    "    llm_calls = state.get(\"llm_calls\", 0)\n",
    "    state[\"llm_calls\"] = llm_calls + 1\n",
    "    print(\"LLM_CALLS:\", state[\"llm_calls\"])\n",
    "    response = model.invoke(input=question)\n",
    "    state[\"answer\"] = response.content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(ChatMessages)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_edge(\"agent\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_CALLS: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Whats the highest mountain in the world?',\n",
       " 'answer': 'Mount Everest',\n",
       " 'llm_calls': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke(input={\"question\": \"Whats the highest mountain in the world?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class InputState(TypedDict):\n",
    "    question: str\n",
    "\n",
    "\n",
    "class PrivateState(TypedDict):\n",
    "    llm_calls: int\n",
    "\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class OverallState(InputState, PrivateState, OutputState):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(OverallState, input=InputState, output=OutputState)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_edge(\"agent\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_CALLS: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'Mount Everest'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"question\": \"Whats the highest mountain in the world?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add runtime configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "def call_model(state: OverallState, config: RunnableConfig):\n",
    "    language = config[\"configurable\"].get(\"language\", \"English\")\n",
    "    system_message_content = \"Respond in {}\".format(language)\n",
    "    system_message = SystemMessage(content=system_message_content)\n",
    "    messages = [system_message, HumanMessage(content=state[\"question\"])]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"answer\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(ChatMessages)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_edge(\"agent\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What's the highest mountain in the world?\",\n",
       " 'answer': AIMessage(content='La montaña más alta del mundo es el Monte Everest, con una altitud de 8,848 metros sobre el nivel del mar. Se encuentra en la cordillera del Himalaya, en la frontera entre Nepal y China.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 23, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5bf40af7-4f7f-4f71-ab8b-9500c94d213c-0', usage_metadata={'input_tokens': 23, 'output_tokens': 49, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"language\": \"Spanish\"}}\n",
    "graph.invoke({\"question\": \"What's the highest mountain in the world?\"}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What's the highest mountain in the world?\",\n",
       " 'answer': AIMessage(content='世界で一番高い山はエベレストです。標高は8,848メートルです。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 23, 'total_tokens': 54, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d709d9d9-e447-45c6-a4aa-0d4d7d1b56cf-0', usage_metadata={'input_tokens': 23, 'output_tokens': 31, 'total_tokens': 54, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"language\": \"Japanese\"}}\n",
    "graph.invoke({\"question\": \"What's the highest mountain in the world?\"}, config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
